#PYTHONHTTPSVERIFY=0 CURL_CA_BUNDLE="" HTTPS_PROXY="localhost:8080" python3
import requests
import urllib3
from dxf import DXFBase, DXF
import traceback
import json
import tarfile
import io
import ntpath
import logging
import uuid

#urllib3.disable_warnings()
#REGISTRY='localhost:5000'
MAX_SIZE = 1024*5
MAX_LAYERS = 10

def get_auth(repo):
    r = requests.get(f"https://auth.docker.io/token?scope=repository%3A{repo}%3Apull&service=registry.docker.io")
    return r.json()['access_token']
    #return r
    



def auth(dxf, response):
    dxf.authenticate('UNAME', 'TOKEN', response=response)
    #dxf.authenticate('', '', response=response)
    dxf.authenticate( response=response)
    print ('in auth:' + response.text)
    #dxf.authenticate(response=get_auth('library/ubuntu'))
    #dxf.authenticate(authorization='Bearer '+ get_auth('library/ubuntu'))
    #return get_auth('library/ubuntu')

def getdfx(repo=""):
    print(repo)
    #return DXF('registry-1.docker.io', repo, auth=None)
    return DXF('registry-1.docker.io', repo, auth=auth)


"""
    #dxf=getdfx(repo='library/ubuntu')
    tags =mydxf.list_aliases()
     for cur_tag in tags:

"""


def scanlayer(layer):
    mydxf =DXF('registry-1.docker.io', repo_name, auth=auth)
    




def analyze_repo(repo_name,tag,writer,mode='analyze',file_to_Extract=None,target_layer=None,max_layers=MAX_LAYERS):
    #logger = open("fs_scan.log", "a")
    #mydxf=getdfx(repo=repo_name)
    #mydxf =DXF('registry-1.docker.io', repo_name, auth=auth)
    #dxf=getdfx(repo='library/ubuntu')
    #tags =mydxf.list_aliases()
    mydxf =DXF('registry-1.docker.io', repo_name, auth=auth)
    #logging.info(dir(mydxf))
    #logging.info("TAGS===="+str(tags))
    tags = mydxf.list_aliases()
    if tag not in tags:
        print(f"[Warning] Tag f{tag} not found..switching to first tag f{tags[0]}")
        tag = tags.pop()

    results=f"###.{repo_name}:{tag}.##\n"
    counter =0 


    manifest=json.loads(str(mydxf.get_manifest(tag)))  #dict if multi arch else str
    #print(str(dxf.get_manifest(cur_tag)))
    if manifest['schemaVersion']==1:
        normalizedFS=manifest['fsLayers']
        normalizedblob="blobSum"
        #continue
    else:
        normalizedFS=reversed(manifest['layers'])
        normalizedblob="digest"
    try:
        if mode == "extract":
            target_layer_flag = False

        for layer in normalizedFS:
            if (mode == "extract") and (target_layer_flag is False):
                if layer[normalizedblob] == target_layer:
                    target_layer_flag = True
                    continue
                else:
                    continue
            counter+=1
            if counter>=max_layers:
                return results
            print(f"===={layer[normalizedblob]} |||| Ver{manifest['schemaVersion']}===")
            results+=f"\n===={layer[normalizedblob]} |||| Ver{manifest['schemaVersion']}===\n"
            if manifest['schemaVersion']==1:
                size=mydxf.blob_size(layer[normalizedblob])
            else:
                size=layer['size']

            if size >MAX_SIZE and file_to_Extract == False:
                continue
            print(f"size ={size}")

            blob=mydxf.pull_blob(layer[normalizedblob],chunk_size=1000000000)
            print("plled Blob")
            data=""
            for chunk in blob:
                data=chunk    
            tar=tarfile.open(fileobj=io.BytesIO(data))
            #tar.list()s
            allfiles=tar.getnames()
            for myfile in allfiles:
                ##Extract Mode
                #print(f"Comparing {myfile}")
                if myfile==file_to_Extract:
                    print("@@@@FOund myfile in :" + layer[normalizedblob])
                    extract_data=tar.extractfile(myfile)
                    #print("[TYPE]"+type(extract_data))
                    return extract_data.read(10000000).decode("utf-8") 
                    return "FOund myfile in :" + layer[normalizedblob]

                
                ## Scavange Mode
                if ntpath.basename(myfile).startswith(".wh."):
                    myfile=myfile.replace(".wh..wh..opq","")
                    myfile=myfile.replace(".wh..wh.","")
                    myfile=myfile.replace(".wh.","")
                    #msg=f"[WHITEOUT]<a href='?name=test&repo={repo_name}&tag={tag}&extract={myfile}' target='_blank'>{myfile}</a>||{repo_name}:{tag}||{layer[normalizedblob]}||version{manifest['schemaVersion']}"
                    #msg=f"[WHITEOUT]<a href='?name=test&repo={repo_name}&tag={tag}&extract={myfile}' target='_blank'>{myfile}</a>"
                    msg =f"[DELETED] {myfile} "
                    if any(match in myfile for match in [".gpg",".pgp","secret","id_rsa"]):
                        #msg+='  <span class="badge badge-secondary badge-danger"> Warning </span>'
                        msg+= " [!!!!!!]"
                    #logger.write(f"{myfile}||{repo_name}:{cur_tag}||{layer[normalizedblob]}||version{manifest['schemaVersion']}\n")
                    logging.info(msg)
                    results+=msg+"\n"
                    print(msg)
                    #entry={'repo':repo_name.replace("/","_"),'tag':cur_tag,'file':myfile,'FSlayer':layer[normalizedblob],'schemaVersion':manifest['schemaVersion']}
                    #results.append(msg)
                    #writer.set(json.dumps({'PartitionKey': entry['repo'], 'RowKey': str(uuid.uuid4()), 'tag' : entry['tag']+"scavanger", 'file' : entry['file'],'FSlayer':entry['FSlayer'],'schemaVersion':entry['schemaVersion']}))
            
        
    except Exception as e:
        print("[Error_FS_Layer]: " + str(e))

    return results
    





#repos=['library/ubuntu']
repos=['projecteverest/kremlin-linux']

try:
    for repo_name in repos:
        analyze_repo(repo_name,'35f74d02be65',None)
        #analyze_repo(repo_name,'12.04',None)
except Exception as e:
    print("Error in repo: " + str(e))                

